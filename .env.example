# Enable local LLM-backed intent parsing (falls back to regex if not available)
DQ_USE_LOCAL_LLM=true

# Ollama settings (local HTTP API)
# Default host is http://127.0.0.1:11434
# Change if your Ollama runs elsewhere
# OLLAMA_HOST=http://127.0.0.1:11434

# Model to use via Ollama (ensure you have pulled it with `ollama pull`)
DQ_LLM_MODEL=llama3.1:8b-instruct

# Alternatively, use llama.cpp with a local GGUF
# DQ_LLM_MODEL_PATH=/models/llama3.1-8b-instruct-q4.gguf
